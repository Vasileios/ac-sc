---
date: 180123
title: AC
type: page
---

#+BEGIN_SRC java
define _foo
#+END_SRC


* Algorithmic Composition

Algorithm is:

- A set of mathematical instructions that must be followed in a fixed
  order, and that, especially if given to a computer, will help to calculate an answer to a mathematical
problem [1].
-  A systematic procedure that produces – in a finite number of steps
  the answer to a question or the solution of a problem [2].
- [...] (especially computing) a set of rules that must be followed
  when solving a particular problem [4]. (Nierhaus 2009 p.2)


Primary principles of automated information
processing can already be found in the 13th century. Through the works of
Charles Babbage and Ada Lovelace,
The history of algorithmic composition has its beginning
shortly after the turn of the first millennium with a system developed by Guido
of Arezzo enabling the generation of melodic material from texts, spans over the application
of algorithmic principles in the developing complex polyphony and is also
found in the “composition machines” of Athanasius Kircher in the Baroque period.
Furthermore, first applications of algorithms for compositional tasks can be found
in the popular “musical dice game” in the 18th century.

Lejaren Hiller at the Experimental Music Studio to Brian Eno’s generative music.

- Markov models

Markov models are for the most part employed in the field of style
imitation, but also, for example by Hiller and Xenakis, for applications of genuine composition.

- Generative grammars

Chomsky's generative grammars
generation of musical structure

- Transition networks

David Cope’s program
“EMI”
“EMI” generates style imitations after having analyzed a sufficient
number of compositions of a particular genre.

http://www.computerhistory.org/atchm/algorithmic-music-david-cope-and-emi/

- chaos and self-similarity

Fractals, Lindenmayer systems

- Genetic algorithms

- Cellular automata

- Neural networks

- Artificial intelligence

Nierhaus 2009 (Algorithmic Composition),

Roads 1996 (Computer Music Tutorial)



** Sound Design

Summarise Farnell 2010, Sigman 2011, Tolonen et al. 1998.

** SuperCollider

From: [[http://supercollider.github.io][SuperCollider]]

** Basics

SuperCollider is a platform for audio synthesis and algorithmic composition, used by musicians, artists, and researchers working with sound. It is free and open source software available for Windows, macOS, and Linux.

** SuperCollider features three major components:

- *scsynth*, a real-time audio server, forms the core of the platform. It features 400+ unit generators ("UGens") for analysis, synthesis, and processing. Its granularity allows the fluid combination of many known and unknown audio techniques, moving between additive and subtractive synthesis, FM, granular synthesis, FFT, and physical modelling. You can write your own UGens in C++, and users have already contributed several hundred more to the sc3-plugins repository.
- *sclang*, an interpreted programming language. It is focused on sound, but not limited to any specific domain. sclang controls scsynth via Open Sound Control. You can use it for algorithmic composition and sequencing, finding new sound synthesis methods, connecting your app to external hardware including MIDI controllers, network music, writing GUIs and visual displays, or for your daily programming experiments. It has a stock of user-contributed extensions called Quarks.
- *scide* is an editor for sclang with an integrated help system.



** Nodes

From: [[https://composerprogrammer.com/teaching/supercollider/sctutorial/6.3%2520Nodes.html][composerprogrammer.com]]

The Server has a graph of all the running Synths, which may be organised into Groups for convenience. You can see Synths and Groups being created just by looking at the Server graphics.

A Node means a Synth or a Group. Whenever you press command+period you reset the graph, cleaning out all the Synths and Groups you added, that is, clearing all Nodes.

The initial state of the Node graph on the Server looks like this (do command+period first to destroy any existing nodes so you have the starting state):

#+BEGIN_SRC js
s.queryAllNodes //run me to see the Nodes on the Server
#+END_SRC

The two default Nodes are convenient Groups for putting your Synths into.

Group(0) is the absolute root of the tree. All new Synths get placed within this Group somewhere (they might be in subGroups but they will be within the RootNode Group at the top of the hierarchy).

#+BEGIN_SRC js
r=RootNode.new; //this gets a reference to Group(0)
#+END_SRC

Group(1) was added as an additional default to receive all created Synths, to avoid cluttering the base of the tree.

#+BEGIN_SRC js
Group.basicNew(s, 1); //this gets a reference to Group(1)
#+END_SRC




** Find recordings folder

#+BEGIN_SRC js
thisProcess.platform.recordingsDir;
#+END_SRC



** Synthesis techniques


Introduction to sound design (origin, definition, procedures, application fields).

#+BEGIN_QUOTE
SuperCollider example:
Creating a sine wave
#+END_QUOTE


#+BEGIN_SRC js
{SinOsc.ar(440, 0, 0.3)}.play
#+END_SRC

#+BEGIN_QUOTE
Additive synthesis
#+END_QUOTE
#+BEGIN_SRC js
{SinOsc.ar(440, 0, 0.4)+SinOsc.ar(660, 0, 0.3)}.play;
#+END_SRC

#+BEGIN_QUOTE
Subtractive synthesis
#+END_QUOTE

#+BEGIN_SRC js
{LPF.ar(Saw.ar(440, 0.4), [3520, 4400, 5280], 0.3)}.play;
#+END_SRC

#+BEGIN_QUOTE
Granular synthesis
#+END_QUOTE

#+BEGIN_SRC js

SynthDef(\granular, {|out = 0, trig = 1, dur = 0.1, sndbuf, pos = 0.2,
rate = 1, pan = 0, amp = 0.4|
var env, source;
env = EnvGen.kr(Env.adsr, 1, doneAcion: 2);
source = Out.ar(out, GrainBuf.ar(2, Impulse.kr(trig), dur, sndbuf, rate, pos, 2,
pan, envbuf) * env)
}).add;

#+END_SRC



** Input Devices


Musical gestures can be expressed through a wide range of body
movements. Dozens of input devices have been developed to capture
these gestures. (Roads 1996: 625)

#+BEGIN_QUOTE
Switch
Push buttons
Linear potentiometer or fader
Trackball
Joystick
Game Paddles
etc
#+END_QUOTE
** Instrument design


[[http://bela.io][Bela]]


Capacitive touch sensor-raspberry pi

[[https://learn.adafruit.com/mpr121-capacitive-touch-sensor-on-raspberry-pi-and-beaglebone-black/overview][MPR121]]




** Mapping the Data from the Input Device

The message coming from digital input devices are streams of binary
numbers. A microprocessor inside the receiving synthesizer must decode
these streams before commanding the synthesis engine  to emit
sound. (Roads 1996: 625)


** Remote Control
   kdnnc
dkncknc
dkcknkndkc
dbccjd

